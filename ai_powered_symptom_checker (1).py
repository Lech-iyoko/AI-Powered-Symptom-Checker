# -*- coding: utf-8 -*-
"""AI-Powered Symptom Checker

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8IoAhxgE-0qSJDDAF7_xr46xq7uJpDT

# Data Collection & Pre-processing
"""

!pip install datasets

from datasets import load_dataset

# Load Data
ds = load_dataset("BI55/MedText")
df = ds['train'].to_pandas()

# Explore Data Structure
print(ds)
print(df.head(20))

!pip install textacy==0.12.0
!pip install spacy
!python -m spacy download en_core_web_sm
!pip install contractions
!pip install unidecode

import textacy
from textacy import preprocessing
import re
import spacy
import contractions
import unicodedata
import pandas as pd
from transformers import BertTokenizer

nlp = spacy.load('en_core_web_sm')

class CleanData:
    def __init__(self, dataframe, columns):
        self.dataframe = dataframe
        self.columns = columns
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

    def normalize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'\s+', ' ', x).strip() if isinstance(x, str) else x)
        return self

    def convert_to_lowercase(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: x.lower() if isinstance(x, str) else x)
        return self

    def remove_stopwords(self, stopwords):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(
                lambda x: ' '.join([word for word in x.split() if word not in stopwords]) if isinstance(x, str) else x
            )
        return self

    def remove_special_characters(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x) if isinstance(x, str) else x)
        return self

    def lemmatize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))
        return self

    def expand_contractions(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: contractions.fix(x))
        return self

    def remove_html_and_urls(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'http\S+|www.\S+', '', x))
        return self

    def remove_accents(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ''.join(c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn'))
        return self

    def tokenize(self):
      for column in self.columns:
        self.dataframe[column] = self.dataframe[column].apply(lambda x: self.tokenizer.tokenize(x) if isinstance(x, str) else x)
      return self


columns = ['Prompt', 'Completion']
cleaner = CleanData(df, columns)
stopwords = [ 'his', 'her', 'is', 'a', 'the', 'and', 'be', 'what', 'with', 'of', 'in', 'he', 'she', 'it', 'they', 'we', 'you', 'i', 'at', 'by', 'for', 'from', 'on', 'to', 'up', 'down', 'over', 'under', 'but', 'or', 'nor', 'so', 'yet', 'an', 'am', 'are', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'shall', 'should', 'can', 'could', 'may', 'might', 'must', 'this', 'that', 'these', 'those', 'there', 'here', 'when', 'where', 'why', 'how', 'which', 'who', 'whom', 'whose', 'because', 'if', 'then', 'than', 'too', 'very', 'just', 'now', 'not', 'only', 'also', 'any', 'each', 'every', 'either', 'neither', 'both', 'all', 'some', 'such', 'no', 'nor', 'more', 'most', 'many', 'much', 'few', 'less', 'least', 'own', 'same', 'so', 'too', 'very' ]
cleaner.normalize().convert_to_lowercase().remove_stopwords(stopwords).remove_special_characters().lemmatize().expand_contractions().remove_html_and_urls().remove_accents()

print(df.head(10))

"""**Preprocessing: Flagging Insufficient Data**

**Key Indicators of Insufficient Data**
1. **Text Length Check**: Inputs with less than a threshold of meaningful tokens (e.g., 5) are flagged as "insufficient."
2. **Medical Keyword Absence**: Use a dictionary of medical terms (e.g., symptoms, conditions) to check relevance. If no terms match, flag the input.
3. **Entity Absence**: Use an NLP library like spaCy to ensure entities such as symptoms, body parts, or medical conditions are present. Inputs without relevant entities are flagged.
"""

# Import the NLP library spacy
import spacy

# Load spaCy model for entity recognition
nlp = spacy.load("en_core_web_sm")

# Medical keyword list
medical_keywords = ["fever", "pain", "cough", "infection", "diagnosis", "symptom", "history", "severe"]

def flag_insufficient_data(text):
    # Text length check
    if len(text.split()) < 5:
        return True, "Text too short"

    # Keyword check
    if not any(keyword in text.lower() for keyword in medical_keywords):
        return True, "No relevant medical keywords"

    # Entity check
    doc = nlp(text)
    entities = [ent.label_ for ent in doc.ents]
    if not any(ent in ["SYMPTOM", "DISEASE", "ORG", "GPE"] for ent in entities):
        return True, "No medical entities found"

    return False, "Sufficient data"

# Test cases
sample_texts = [
    "Patient feels unwell.",
    "The patient presents with a high fever and severe pain.",
    "What are you?"
]

for text in sample_texts:
    flag, reason = flag_insufficient_data(text)
    print(f"Text: {text} | Flagged: {flag} | Reason: {reason}")

"""# Exploratory Data Analysis"""

# Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from sentence_transformers import SentenceTransformer
import spacy

# Text and Token Length distribution
df['Prompt_length'] = df['Prompt'].apply(lambda x: len(x.split()))
df['Completion_length'] = df['Completion'].apply(lambda x: len(x.split()))

# Plot distribution
plt.figure(figsize=(12, 6))
sns.histplot(df['Prompt_length'], bins=30, kde=True, label="Prompt Length")
sns.histplot(df['Completion_length'], bins=30, kde=True, label="Completion Length", color='red')
plt.legend()
plt.title("Distribution of Text Lengths")
plt.show()

# Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

def extract_entities(text):
    doc = nlp(text)
    return [ent.text for ent in doc.ents]

df['Prompt_Entities'] = df['Prompt'].apply(extract_entities)
df['Completion_Entities'] = df['Completion'].apply(extract_entities)

# Check some examples
df[['Prompt', 'Prompt_Entities', 'Completion', 'Completion_Entities']].head()

# Generate Sentence Embeddings for Prompt(Symptoms)
# Load Sentence Transformer model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Generate embeddings for Prompt
prompt_embeddings = model.encode(df['Prompt'].tolist(), convert_to_tensor=True)

# Store embeddings
df['Prompt_Embeddings'] = list(prompt_embeddings.cpu().numpy())

# Explore Relationships in Data (Embeddings & Similarity)
from sklearn.metrics.pairwise import cosine_similarity

# Compute cosine similarity
similarity_matrix = cosine_similarity(prompt_embeddings.cpu().numpy())

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(similarity_matrix[:50, :50], cmap="coolwarm", annot=False)
plt.title("Cosine Similarity of Symptom Descriptions")
plt.show()

"""# Class Consolidation"""

!pip install umap-learn
!pip install hdbscan

# Generate embeddings for Completion (Diagnoses/Outcomes)
completion_embeddings = model.encode(df['Completion'].tolist(), convert_to_tensor=True)

# Store embeddings
df['Completion_Embeddings'] = list(completion_embeddings.cpu().numpy())

# Group similar completion classes
import umap
import hdbscan
from sklearn.metrics import silhouette_score

# Reduce dimensionality with UMAP
umap_embeddings = umap.UMAP(n_components=2, random_state=42).fit_transform(completion_embeddings.cpu().numpy())

# Apply HDBSCAN
hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=3)
df['Cluster_HDBSCAN'] = hdbscan_cluster.fit_predict(umap_embeddings)

# Compute Silhouette Score
silhouette_hdbscan = silhouette_score(umap_embeddings, df['Cluster_HDBSCAN'])
print(f"Silhouette Score for HDBSCAN: {silhouette_hdbscan}")

# Visualize clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], hue=df['Cluster_HDBSCAN'], palette="magma")
plt.title("HDBSCAN Clustering Visualization (UMAP)")
plt.show()

# Display sample cases from each HDBSCAN cluster
for cluster in df['Cluster_HDBSCAN'].unique():
    print(f"\nCluster {cluster}:")
    print(df[df['Cluster_HDBSCAN'] == cluster]['Completion'].sample(5, random_state=42).tolist())

print("Number of noise points (Cluster -1):", (df['Cluster_HDBSCAN'] == -1).sum())

"""# Model Prototyping

"""

!pip install transformers datasets torch accelerate

from datasets import Dataset
from transformers import AutoTokenizer

# Load dataset into Hugging Face Dataset format
dataset = Dataset.from_pandas(df[['Prompt', 'Cluster_HDBSCAN']])

# Load BERT tokenizer (Use 'dmis-lab/biobert-base-cased-v1.1' for BioBERT)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize dataset
def tokenize_function(example):
    return tokenizer(example["Prompt"], padding="max_length", truncation=True)

dataset = dataset.map(tokenize_function, batched=True)

from sklearn.model_selection import train_test_split

# Now split the tokenized dataset using train_test_split
train_dataset, test_dataset = train_test_split(
    dataset,
    test_size=0.2,
    stratify=dataset['Cluster_HDBSCAN'],
    random_state=42
)


# Convert back to Hugging Face dataset format
train_dataset = Dataset.from_pandas(train_dataset)
test_dataset = Dataset.from_pandas(test_dataset)

from transformers import AutoModelForSequenceClassification

# Load pretrained BERT model
num_labels = df['Cluster_HDBSCAN'].nunique()  # Number of clusters as output labels
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)

from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./bert_symptom_checker",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=500,
    report_to="none"
)

# Define trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

# Train model
trainer.train()

trainer.evaluate()

# Import the necessary libraries to train the baseline models
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Split the data into train and test sets
X = df['Prompt'] # Symptoms
y = df['Cluster_HDBSCAN'] # Outcomes

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the TF-IDF vecotorizer
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)

# Fit and transform the training data
X_train_tfidf = vectorizer.fit_transform(X_train)

# Transform the test data
X_test_tfidf = vectorizer.transform(X_test)

from sklearn.svm import SVC

clf = SVC(kernel='linear', C=1.0, random_state=42)
clf.fit(X_train_tfidf, y_train)  # Use Simplified Labels Here!
y_pred = clf.predict(X_test_tfidf)

print(classification_report(y_test, y_pred))

# Print classification report
print(classification_report(y_test, y_pred))

# Evaluating the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, y_pred, average='weighted'))

print(y_train.value_counts())

print(y_train.value_counts())
print(y_test.value_counts())

# Evaluate model performance
report = classification_report(y_test, y_pred)
print(report)

from sklearn.metrics import confusion_matrix

print(confusion_matrix(y_test, y_pred))