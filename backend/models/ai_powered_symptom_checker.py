# -*- coding: utf-8 -*-
"""AI-Powered Symptom Checker

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8IoAhxgE-0qSJDDAF7_xr46xq7uJpDT

# Data Collection & Pre-processing
"""

!pip install datasets

from datasets import load_dataset

# Load Data
ds = load_dataset("BI55/MedText")
df = ds['train'].to_pandas()

# Explore Data Structure
print(ds)
print(df.head(20))

# Optionally reduce the dataset size to ease memory requirements:
df = df.sample(frac=0.5, random_state=42)

!pip install spacy
!python -m spacy download en_core_web_sm
!pip install contractions
!pip install unidecode

# Install scispacy and the scientific model
!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz

import re
import spacy
import contractions
import unicodedata
import pandas as pd
from transformers import BertTokenizer

nlp = spacy.load("en_core_sci_sm")

class CleanData:
    def __init__(self, dataframe, columns, stopwords):
        self.dataframe = dataframe
        self.columns = columns
        self.stopwords = stopwords or []
        self.tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')

    def handle_missing_data(self, fill_value=""):
      for column in self.columns:
        self.dataframe[column] = self.dataframe[column].fillna(fill_value).astype(str)
      return self

    def normalize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'\s+', ' ', x).strip() if isinstance(x, str) else x)
        return self

    def convert_to_lowercase(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: x.lower() if isinstance(x, str) else x)
        return self

    def remove_stopwords(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(
                lambda x: ' '.join([word for word in x.split() if word not in self.stopwords])
            )
        return self

    def remove_special_characters(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x) if isinstance(x, str) else x)
        return self

    def lemmatize(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))
        return self

    def expand_contractions(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: contractions.fix(x))
        return self

    def remove_html_and_urls(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: re.sub(r'http\S+|www.\S+', '', x))
        return self

    def remove_accents(self):
        for column in self.columns:
            self.dataframe[column] = self.dataframe[column].apply(lambda x: ''.join(c for c in unicodedata.normalize('NFD', x) if unicodedata.category(c) != 'Mn'))
        return self

    def tokenize(self):
      for column in self.columns:
        self.dataframe[column] = self.dataframe[column].apply(lambda x: self.tokenizer.tokenize(x) if isinstance(x, str) else x)
      return self


columns = ['Prompt', 'Completion']
stopwords = ['the', 'and', 'is', 'a']
cleaner = CleanData(df, columns, stopwords=stopwords)
cleaner.handle_missing_data().normalize().convert_to_lowercase().remove_stopwords()

print(df.head(10))

"""**Preprocessing: Flagging Insufficient Data**

**Key Indicators of Insufficient Data**
1. **Text Length Check**: Inputs with less than a threshold of meaningful tokens (e.g., 5) are flagged as "insufficient."
2. **Medical Keyword Absence**: Use a dictionary of medical terms (e.g., symptoms, conditions) to check relevance. If no terms match, flag the input.
3. **Entity Absence**: Use an NLP library like spaCy to ensure entities such as symptoms, body parts, or medical conditions are present. Inputs without relevant entities are flagged.
"""

# Load the scispacy model
nlp = spacy.load("en_core_sci_sm")

# Example text
text = "The patient presents with a high fever, severe headache, and a history of hypertension."

# Process the text
doc = nlp(text)

# Extract entities
print("Entities:")
for ent in doc.ents:
    print(f"Text: {ent.text}, Label: {ent.label_}")

# Extract tokens and their attributes
print("\nTokens:")
for token in doc:
    print(f"Text: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}")

# Medical keyword list (can be loaded from a file or ontology)
medical_keywords = ["fever", "pain", "cough", "infection", "diagnosis", "symptom", "history", "severe"]

def flag_insufficient_data(text, min_words=5, medical_keywords=medical_keywords, entity_labels=["SYMPTOM", "DISEASE"]):
    """
    Flags insufficient data based on text length, medical keywords, and entities.

    Args:
        text (str): Input text.
        min_words (int): Minimum number of words required.
        medical_keywords (list): List of medical keywords to check.
        entity_labels (list): List of entity labels to check.

    Returns:
        tuple: (bool, str) indicating whether the text is flagged and the reason.
    """
    # Text length check
    if len(text.split()) < min_words:
        return True, "Text too short"

    # Keyword check
    if not any(keyword in text.lower() for keyword in medical_keywords):
        return True, "No relevant medical keywords"

    # Entity check
    doc = nlp(text)
    entities = [ent.label_ for ent in doc.ents]
    if not any(ent in entity_labels for ent in entities):
        return True, "No medical entities found"

    return False, "Sufficient data"

# Test cases
sample_texts = [
    "Patient feels unwell.",
    "The patient presents with a high fever and severe pain.",
    "What are you?"
]

for text in sample_texts:
    flag, reason = flag_insufficient_data(text)
    print(f"Text: {text} | Flagged: {flag} | Reason: {reason}")

"""# Exploratory Data Analysis"""

# Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from sentence_transformers import SentenceTransformer

# Text and Token Length distribution
df['Prompt_length'] = df['Prompt'].apply(lambda x: len(x.split()))
df['Completion_length'] = df['Completion'].apply(lambda x: len(x.split()))

# Plot distribution
plt.figure(figsize=(12, 6))
sns.histplot(df['Prompt_length'], bins=30, kde=True, label="Prompt Length")
sns.histplot(df['Completion_length'], bins=30, kde=True, label="Completion Length", color='red')
plt.legend()
plt.title("Distribution of Text Lengths")
plt.show()

# Named Entity Recognition
nlp = spacy.load("en_core_sci_sm")

def extract_entities(text):
    doc = nlp(text)
    return [ent.text for ent in doc.ents]

df['Prompt_Entities'] = df['Prompt'].apply(extract_entities)
df['Completion_Entities'] = df['Completion'].apply(extract_entities)

# Check some examples
df[['Prompt', 'Prompt_Entities', 'Completion', 'Completion_Entities']].head()

# Generate Sentence Embeddings for Prompt(Symptoms)
# Load Sentence Transformer model
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Generate embeddings for Prompt
prompt_embeddings = model.encode(df['Prompt'].tolist(), convert_to_tensor=True)

# Store embeddings
df['Prompt_Embeddings'] = list(prompt_embeddings.cpu().numpy())

# Explore Relationships in Data (Embeddings & Similarity)
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(prompt_embeddings.cpu().numpy())

# Plot Symptom Embeddings
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
plt.title("2D PCA of Symptom Embeddings")
plt.show()

"""# Feature Engineering


"""

!pip install umap-learn
!pip install hdbscan

# Generate embeddings for Completion (Diagnoses/Outcomes)
completion_embeddings = model.encode(df['Completion'].tolist(), convert_to_tensor=True)

# Store embeddings
df['Completion_Embeddings'] = list(completion_embeddings.cpu().numpy())

# Group similar completion classes
import umap
import hdbscan
from sklearn.metrics import silhouette_score

# Reduce dimensionality with UMAP
umap_embeddings = umap.UMAP(n_components=2, random_state=42).fit_transform(completion_embeddings.cpu().numpy())

# Apply HDBSCAN
hdbscan_cluster = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=3)
df['Cluster_HDBSCAN'] = hdbscan_cluster.fit_predict(umap_embeddings)

# Compute Silhouette Score
silhouette_hdbscan = silhouette_score(umap_embeddings, df['Cluster_HDBSCAN'])
print(f"Silhouette Score for HDBSCAN: {silhouette_hdbscan}")

# Visualize clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=umap_embeddings[:, 0], y=umap_embeddings[:, 1], hue=df['Cluster_HDBSCAN'], palette="magma")
plt.title("HDBSCAN Clustering Visualization (UMAP)")
plt.show()

# Display sample cases from each HDBSCAN cluster
for cluster in df['Cluster_HDBSCAN'].unique():
    print(f"\nCluster {cluster}:")
    # Get the cluster data
    cluster_data = df[df['Cluster_HDBSCAN'] == cluster]['Completion']
    # Sample a maximum of 5 elements or the cluster size, whichever is smaller
    sample_size = min(5, len(cluster_data))
    print(cluster_data.sample(sample_size, random_state=42).tolist())

print("Number of noise points (Cluster -1):", (df['Cluster_HDBSCAN'] == -1).sum())

from datasets import Dataset  # Import the Dataset class

# Option 1: More comprehensive filtering
print("Before filtering:", len(df))  # Changed 'dataset' to 'df'
dataset = Dataset.from_pandas(df)  # Create a Hugging Face Dataset from 'df'
dataset = dataset.filter(lambda example: example['Cluster_HDBSCAN'] != -1)  # Filter noise points
print("After filtering:", len(dataset))

"""# Model Training and Evaluation

"""

!pip install transformers datasets torch accelerate

from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load dataset into Hugging Face Dataset format
dataset = Dataset.from_pandas(df[['Prompt', 'Cluster_HDBSCAN']])

# Load BERT tokenizer (Use 'dmis-lab/biobert-base-cased-v1.1' for BioBERT)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize dataset
def tokenize_function(examples):
    # Convert 'Cluster_HDBSCAN' to a list of strings
    # This is crucial for batched processing
    text_pairs = [str(cluster) for cluster in examples["Cluster_HDBSCAN"]]

    # Tokenize using batch_text with text and text_pair lists
    return tokenizer(
        examples["Prompt"],
        padding="max_length",
        truncation=True,
        text_pair=text_pairs, # Pass the list of text_pairs
        return_tensors="pt"
    )

#include labels during tokenization and formatting
dataset = dataset.map(tokenize_function, batched=True)

#rename Cluster_HDBSCAN to labels
dataset = dataset.rename_column("Cluster_HDBSCAN", "labels")

from sklearn.model_selection import train_test_split

# Convert to pandas DataFrame
df_split = dataset.to_pandas()

# Filter out noise points (labels == -1)
df_filtered = df_split[df_split['labels'] != -1]
print("After filtering noise:", len(df_filtered))

# Split using the filtered DataFrame
train_df, test_df = train_test_split(
    df_filtered,
    test_size=0.2,
    stratify=df_filtered['labels'],
    random_state=42
)

# Convert back to Hugging Face Dataset
from datasets import Dataset
train_dataset = Dataset.from_pandas(train_df, preserve_index=False)
test_dataset = Dataset.from_pandas(test_df, preserve_index=False)

# Update num_labels based on training data
num_labels = train_df['labels'].nunique()

# Load pretrained BERT model
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels
)

# Enable gradient checkpointing to reduce memory consumption
model.gradient_checkpointing_enable()

!nvidia-smi  # Check if GPU is being used

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
    output_dir="./bert_symptom_checker",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=2,  # Reduced from 8
    per_device_eval_batch_size=2,   # Reduced from 8
    num_train_epochs=5,
    learning_rate=2e-5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=50,
    report_to="none"
)

# Define trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer
)

import gc
import torch

gc.collect()
torch.cuda.empty_cache()

# Train model
trainer.train()

!pip install evaluate

from evaluate import load  # Correct import from the evaluate library

# Load the accuracy metric
accuracy_metric = load("accuracy")

# Get predictions
results = trainer.predict(test_dataset)
predictions = results.predictions.argmax(-1)
labels = results.label_ids

# Compute accuracy
accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
print("Accuracy:", accuracy)

# Load and compute F1-score
f1_metric = load("f1")
f1_score = f1_metric.compute(predictions=predictions, references=labels, average="weighted")
print("F1-score:", f1_score)

# Load precision and recall metrics
precision_metric = load("precision")
recall_metric = load("recall")

# Compute precision
precision = precision_metric.compute(predictions=predictions, references=labels, average="weighted")

# Compute recall
recall = recall_metric.compute(predictions=predictions, references=labels, average="weighted")

print("Precision:", precision)
print("Recall:", recall)

# Define the directory where the model and tokenizer are saved
model_dir = "/content/bert_symptom_checker"

# Save model and tokenizer
model.save_pretrained(model_dir, safe_serialization=False)
tokenizer.save_pretrained(model_dir)

# Try saving with explicit verbose output
print("Saving model...")
model.save_pretrained(model_dir, safe_serialization=False)
print("Save complete.")
print("Files in directory:", os.listdir(model_dir))

# Check if config.json exists (should always be present)
if "config.json" in os.listdir(model_dir):
    print("config.json exists, checking contents...")
    with open(os.path.join(model_dir, "config.json"), "r") as f:
        print(f.read()[:100])  # Print first 100 chars
else:
    print("config.json missing!")

import os

# Define the required files
required_files = [
    "pytorch_model.bin",
    "model.safetensors",
    "tf_model.h5",
    "model.ckpt",
    "flax_model.msgpack",
    "config.json",
    "tokenizer_config.json",
    "vocab.txt",  # or other tokenizer files
    "special_tokens_map.json",
    "tokenizer.json"
]

# Function to check for required files in a directory
def check_files_in_directory(directory, required_files):
    files = os.listdir(directory)
    missing_files = [file for file in required_files if file not in files]
    return missing_files

# Check the main directory
missing_files = check_files_in_directory(model_dir, required_files)
if not missing_files:
    print(f"All required files are present in {model_dir}.")
else:
    print(f"Missing files in {model_dir}: {missing_files}")

# Check subdirectories
for subdir in os.listdir(model_dir):
    subdir_path = os.path.join(model_dir, subdir)
    if os.path.isdir(subdir_path):
        missing_files = check_files_in_directory(subdir_path, required_files)
        if not missing_files:
            print(f"All required files are present in {subdir_path}.")
        else:
            print(f"Missing files in {subdir_path}: {missing_files}")

!pip install huggingface_hub

from huggingface_hub import notebook_login
notebook_login()

from huggingface_hub import HfApi

api = HfApi()

# Create the repository (or use existing one)
try:
    api.create_repo(repo_id="Lech-Iyoko/bert-symptom-checker", repo_type="model")
    print("Repository created successfully")
except Exception as e:
    print(f"Repository already exists or error occurred: {e}")

# Upload only the necessary files (ignore checkpoints)
essential_files = [
    "/content/bert_symptom_checker/pytorch_model.bin",
    "/content/bert_symptom_checker/model.safetensors",
    "/content/bert_symptom_checker/config.json",
    "/content/bert_symptom_checker/tokenizer_config.json",
    "/content/bert_symptom_checker/vocab.txt",
    "/content/bert_symptom_checker/special_tokens_map.json",
    "/content/bert_symptom_checker/tokenizer.json"
]

# Check if files exist before uploading
import os
files_to_upload = [f for f in essential_files if os.path.exists(f)]
print(f"Uploading {len(files_to_upload)} files...")

# Upload files individually
for file_path in files_to_upload:
    file_name = os.path.basename(file_path)
    try:
        api.upload_file(
            path_or_fileobj=file_path,
            path_in_repo=file_name,
            repo_id="Lech-Iyoko/bert-symptom-checker",
            repo_type="model"
        )
        print(f"Uploaded {file_name}")
    except Exception as e:
        print(f"Error uploading {file_name}: {e}")

print("Upload complete!")